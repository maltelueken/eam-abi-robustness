{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numba.random'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jit, njit, prange\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numba.random'"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "import os\n",
    "if \"KERAS_BACKEND\" not in os.environ:\n",
    "    # set this to \"torch\", \"tensorflow\", or \"jax\"\n",
    "    os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "import numpy as np\n",
    "import bayesflow as bf\n",
    "import keras\n",
    "from numba import jit, njit, prange\n",
    "import numba.random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNG = np.random.default_rng(2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulator_fun():\n",
    "    num_accumulators = 2\n",
    "    num_obs = 500\n",
    "    dt = 1\n",
    "    t_max = 1_000\n",
    "    t = np.linspace(dt, t_max, int(t_max/dt))\n",
    "    amp = 20\n",
    "    tau = RNG.gamma(12, 6)\n",
    "    a_shape = 2\n",
    "    b = 100\n",
    "    t0 = 300\n",
    "    s = 4\n",
    "    mu_c = RNG.gamma(1, 1, size=2)\n",
    "\n",
    "    eq4 = (\n",
    "        amp\n",
    "        * np.exp(-t / tau)\n",
    "        * (np.exp(1) * t / (a_shape - 1) / tau)\n",
    "        ** (a_shape - 1)\n",
    "    )\n",
    "\n",
    "    mu = mu_c[:, None] + eq4 * ((a_shape - 1) / t - 1 / tau)\n",
    "\n",
    "    # fpt = np.zeros((batch_size, num_accumulators, num_obs))\n",
    "    \n",
    "    xt = mu[:, None] + (s * RNG.normal(size=(num_accumulators, num_obs, t_max)))\n",
    "    fpt = t[(np.cumsum(xt, axis=2) > b).argmax(axis=2)]\n",
    "\n",
    "    resp = fpt.argmin(axis=0)\n",
    "    rt = (fpt.min(axis=0) + t0)\n",
    "\n",
    "    return {\"x\": np.c_[rt, resp], \"mu_c\": mu_c, \"tau\": tau}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.4 ms ± 125 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "simulator_fun()[\"x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def simulator_fun():\n",
    "    num_accumulators = 2\n",
    "    num_obs = 500\n",
    "    dt = 1.0\n",
    "    t_max = 1000\n",
    "    t = np.linspace(dt, t_max, int(t_max / dt))\n",
    "    amp = 20.0\n",
    "    tau = RNG.gamma(12, 6)  # Random tau from gamma distribution\n",
    "    a_shape = 2.0\n",
    "    b = 100.0\n",
    "    t0 = 300.0\n",
    "    s = 4.0\n",
    "    mu_c = RNG.gamma(1, 1, size=num_accumulators)\n",
    "\n",
    "    eq4 = (\n",
    "        amp\n",
    "        * np.exp(-t / tau)\n",
    "        * (np.exp(1) * t / (a_shape - 1) / tau) ** (a_shape - 1)\n",
    "    )\n",
    "\n",
    "    mu = mu_c[:, None] + eq4 * ((a_shape - 1) / t - 1 / tau)\n",
    "\n",
    "    # Initialize arrays for results\n",
    "    fpt = np.zeros((num_accumulators, num_obs))\n",
    "    xt = np.zeros((num_accumulators, num_obs, t_max))\n",
    "\n",
    "    # Fill the xt array with random values\n",
    "    for i in range(num_accumulators):\n",
    "        xt[i, :, :] = mu[i, None, :] + (s * RNG.standard_normal(size=(num_obs, t_max)))\n",
    "\n",
    "    # Compute fpt manually\n",
    "    for i in range(num_accumulators):\n",
    "        for j in range(num_obs):\n",
    "            cumulative_sum = 0.0\n",
    "            for k in range(t_max):\n",
    "                cumulative_sum += xt[i, j, k]\n",
    "                if cumulative_sum > b:\n",
    "                    fpt[i, j] = t[k]\n",
    "                    break\n",
    "\n",
    "    # Compute resp and rt\n",
    "    resp = np.empty(num_obs, dtype=np.int64)\n",
    "    rt = np.empty(num_obs)\n",
    "\n",
    "    for j in range(num_obs):\n",
    "        min_fpt = np.inf\n",
    "        min_idx = -1\n",
    "        for i in range(num_accumulators):\n",
    "            if fpt[i, j] < min_fpt:\n",
    "                min_fpt = fpt[i, j]\n",
    "                min_idx = i\n",
    "        resp[j] = min_idx\n",
    "        rt[j] = min_fpt + t0\n",
    "\n",
    "    return {\"x\": rt, \"mu_c\": mu_c, \"tau\": tau}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "NumbaNotImplementedError",
     "evalue": "Failed in nopython mode pipeline (step: native lowering)\n\u001b[1m\u001b[1m<numba.core.base.OverloadSelector object at 0x7cb03417d150>, (NumPyRandomGeneratorType,)\u001b[0m\n\u001b[0m\u001b[1mDuring: lowering \"$44load_global.15 = global(RNG: Generator(PCG64))\" at /tmp/ipykernel_191116/3431294076.py (9)\u001b[0m",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNumbaNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msimulator_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repositories/eam-abi-robustness/lib/python3.10/site-packages/numba/core/dispatcher.py:423\u001b[0m, in \u001b[0;36m_DispatcherBase._compile_for_args\u001b[0;34m(self, *args, **kws)\u001b[0m\n\u001b[1;32m    419\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mrstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThis error may have been caused \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    420\u001b[0m                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby the following argument(s):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00margs_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    421\u001b[0m         e\u001b[38;5;241m.\u001b[39mpatch_message(msg)\n\u001b[0;32m--> 423\u001b[0m     \u001b[43merror_rewrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtyping\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mUnsupportedError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;66;03m# Something unsupported is present in the user code, add help info\u001b[39;00m\n\u001b[1;32m    426\u001b[0m     error_rewrite(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munsupported_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Repositories/eam-abi-robustness/lib/python3.10/site-packages/numba/core/dispatcher.py:364\u001b[0m, in \u001b[0;36m_DispatcherBase._compile_for_args.<locals>.error_rewrite\u001b[0;34m(e, issue_type)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mNumbaNotImplementedError\u001b[0m: Failed in nopython mode pipeline (step: native lowering)\n\u001b[1m\u001b[1m<numba.core.base.OverloadSelector object at 0x7cb03417d150>, (NumPyRandomGeneratorType,)\u001b[0m\n\u001b[0m\u001b[1mDuring: lowering \"$44load_global.15 = global(RNG: Generator(PCG64))\" at /tmp/ipykernel_191116/3431294076.py (9)\u001b[0m"
     ]
    }
   ],
   "source": [
    "simulator_fun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = bf.simulators.CompositeLambdaSimulator(sample_fns=[simulator_fun])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_batch = simulator.sample((10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_adapter = bf.ContinuousApproximator.build_data_adapter(\n",
    "    inference_variables=[\"mu_c\", \"tau\"],\n",
    "    # inference_conditions=[\"num_obs\"],\n",
    "    summary_variables=[\"x\"],\n",
    "    transforms=[\n",
    "        bf.data_adapters.transforms.Standardize([\"mu_c\", \"tau\"])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_network = bf.networks.SetTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_network = bf.networks.FlowMatching(\n",
    "    subnet=\"mlp\",\n",
    "    subnet_kwargs=dict(\n",
    "        depth=6,\n",
    "        width=256,\n",
    "    ),\n",
    "    use_optimal_transport=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "approximator = bf.ContinuousApproximator(\n",
    "    summary_network=summary_network,\n",
    "    inference_network=inference_network,\n",
    "    data_adapter=data_adapter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "approximator.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bayesflow:Building dataset from simulator instance of CompositeLambdaSimulator.\n",
      "INFO:bayesflow:Using 32 data loading workers.\n",
      "INFO:bayesflow:Building on a test batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m135/500\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:23\u001b[0m 887ms/step - loss: 1.5400 - loss/inference_loss: 1.5400"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mapproximator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# memory_budget=\"8 GiB\",\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43msimulator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimulator\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repositories/eam-abi-robustness/lib/python3.10/site-packages/bayesflow/approximators/continuous_approximator.py:109\u001b[0m, in \u001b[0;36mContinuousApproximator.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_adapter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repositories/eam-abi-robustness/lib/python3.10/site-packages/bayesflow/approximators/approximator.py:84\u001b[0m, in \u001b[0;36mApproximator.fit\u001b[0;34m(self, dataset, simulator, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m     mock_data \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39mmap_structure(keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mconvert_to_tensor, mock_data)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_from_data(mock_data)\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repositories/eam-abi-robustness/lib/python3.10/site-packages/bayesflow/approximators/backend_approximators/backend_approximator.py:22\u001b[0m, in \u001b[0;36mBackendApproximator.fit\u001b[0;34m(self, dataset, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, dataset: keras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mPyDataset, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfilter_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repositories/eam-abi-robustness/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Repositories/eam-abi-robustness/lib/python3.10/site-packages/keras/src/backend/jax/trainer.py:407\u001b[0m, in \u001b[0;36mJAXTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    404\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jax_state_synced \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 407\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;66;03m# Callbacks\u001b[39;00m\n\u001b[1;32m    409\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;66;03m# Train step\u001b[39;00m\n",
      "File \u001b[0;32m~/Repositories/eam-abi-robustness/lib/python3.10/site-packages/keras/src/trainers/epoch_iterator.py:109\u001b[0m, in \u001b[0;36mEpochIterator.enumerate_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(buffer) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, buffer\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator()):\n\u001b[1;32m    110\u001b[0m         buffer\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution:\n",
      "File \u001b[0;32m~/Repositories/eam-abi-robustness/lib/python3.10/site-packages/keras/src/backend/jax/trainer.py:1012\u001b[0m, in \u001b[0;36mJAXEpochIterator._prefetch_numpy_iterator\u001b[0;34m(self, numpy_iterator)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m queue:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m queue\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[0;32m-> 1012\u001b[0m     \u001b[43menqueue\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repositories/eam-abi-robustness/lib/python3.10/site-packages/keras/src/backend/jax/trainer.py:1006\u001b[0m, in \u001b[0;36mJAXEpochIterator._prefetch_numpy_iterator.<locals>.enqueue\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21menqueue\u001b[39m(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m-> 1006\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mislice(numpy_iterator, n):\n\u001b[1;32m   1007\u001b[0m         queue\u001b[38;5;241m.\u001b[39mappend(_distribute_data(data))\n",
      "File \u001b[0;32m~/Repositories/eam-abi-robustness/lib/python3.10/site-packages/keras/src/trainers/data_adapters/data_adapter_utils.py:181\u001b[0m, in \u001b[0;36mget_jax_iterator\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_jax_iterator\u001b[39m(iterable):\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_to_tensor\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mmap_structure(convert_to_tensor, batch)\n",
      "File \u001b[0;32m~/Repositories/eam-abi-robustness/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:260\u001b[0m, in \u001b[0;36mPyDatasetAdapter._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpy_dataset\u001b[38;5;241m.\u001b[39mnum_batches\n\u001b[1;32m    259\u001b[0m gen_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_multiprocessed_generator_fn()\n\u001b[0;32m--> 260\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(gen_fn()):\n\u001b[1;32m    261\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_standardize_batch(batch)\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m batch\n",
      "File \u001b[0;32m~/Repositories/eam-abi-robustness/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:582\u001b[0m, in \u001b[0;36mOrderedEnqueuer.get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value  \u001b[38;5;66;03m# Propagate exception from other thread\u001b[39;00m\n\u001b[0;32m--> 582\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueue\u001b[38;5;241m.\u001b[39mtask_done()\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = approximator.fit(\n",
    "    epochs=5,\n",
    "    num_batches=500,\n",
    "    batch_size=64,\n",
    "    # memory_budget=\"8 GiB\",\n",
    "    simulator=simulator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesflow_plots import plot_z_score_contraction, plot_recovery\n",
    "from utils import convert_samples_posterior, convert_samples_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = [\"v_intercept\", \"v_slope\", \"s_true\", \"b\", \"t0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_dict = simulator.sample(\n",
    "    batch_shape=(100,), num_obs=np.tile([500], (100,))\n",
    ")\n",
    "\n",
    "prior_samples = convert_samples_prior(forward_dict, param_names)\n",
    "\n",
    "sample_dict = {k: v for k, v in forward_dict.items() if k not in data_adapter.keys[\"inference_variables\"]}\n",
    "\n",
    "posterior_samples_sens = convert_samples_posterior(approximator.sample(\n",
    "    conditions=sample_dict, num_samples=100\n",
    "), param_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_z_score_contraction(posterior_samples_sens, prior_samples, param_names=param_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_recovery(np.swapaxes(posterior_samples_sens, 0, 1), prior_samples, param_names=param_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eam-abi-robustness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
